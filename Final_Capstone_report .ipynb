{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Introduction : Business problem \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Car accidents are one of biggest causes of death among humans nowadays. In order to record every car collision and estimate its severity automatically, ( as it would be easier ) and without any type of human subjectivity, a machine learning model can be used. \n",
    "Given some information as parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1 Data Sources \n",
    "We will use the data provided by the course on coursera. it can be downloaded from [here](https://s3.us.cloud-object-storage.appdomain.cloud/cf-courses-data/CognitiveClass/DP0701EN/version-2/Data-Collisions.csv). And metadata can be found [here](https://s3.us.cloud-object-storage.appdomain.cloud/cf-courses-data/CognitiveClass/DP0701EN/version-2/Metadata.pdf)\n",
    "\n",
    "This is a weekly updated data  of all collisions provided by SPD and recorded by Traffic Records that includes all types of collisions displayed at intersection, mid-block or segment since 2004 until present\n",
    "\n",
    "## 2-2 Data Cleaning, Feature Selection, data Preparation.\n",
    "\n",
    "The recieved data contains a lot of missing values and is not well encoded at all for a machine learning algorithm.  \n",
    "\n",
    "The first step in cleaning the data is checking the description of every columns provided in the metadata. We will then keep only one columns from every linearly dependant set of columns. In fact, we found a repetition in the SEVERITYCODE column and many couple of coluns where one hold the codes and the other holds the description of the code. \n",
    "\n",
    "We will also drop all the features that we judge unnecessary for machine the model. Such as the report number of every collision.  \n",
    "\n",
    "We found some duplicate rows that we deleted and kept only one instance of each unique row. \n",
    "\n",
    "Then we had to deal with missing values. We first dropped all features that had almost only missing values (such as speed). Then, with a simple calculation we found that if we drop all the rows containing at least one missing value, we would delete only 7% of the data. As the dataset size is huge, even with 7% less we would have enough data to build strong models. So, we decided to drop every row containing at least one missing value instead of replacing the missing value with an approximation that may hinder the results. \n",
    "\n",
    "Now that we don't have any missing value, we will deal with the remaining data subset.\n",
    "\n",
    "As known, the categorical dta are not suitable for machine learning algorithms. We used one hot encoder to convert them into numerical data. Before this, we made sure that every features values were well formatted and that there were no similar values in different features.  We had to fix some issues to normalize the forrmat and modify the similar values ( such as 'Unknown' and 'Other' ) \n",
    "\n",
    "We extracted the hour from the INCDTTM feature, we judges that it may be important to know at what hour the collision occured. We think that we could add a binary variable that communicates whether the incident occured at night or not, but we did not. \n",
    "\n",
    "We also added a binary feature that says whether a pedestrian ( cycles or not ) was concerned in the incident or not. We think that incidents concerning pedestrians are more severe.\n",
    "\n",
    "And to finish, we replaced the values [1,2] of SEVERITYCODE by [0,1]. This would be easier with some metrics ( as F1-score ) and it's better-presented.\n",
    "\n",
    "And by then, We normalized the data with Standard Scaler and finally, it was ready for modeling. \n",
    "\n",
    "By the end of this preparation the data was (>180000,82) shaped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Modeling \n",
    "\n",
    "For the model we combined 4 popular models from Scikit-Learn. K-Nearest-Neighbors, RandomForestClassifier, Logistic Regression, Decision Tree. \n",
    "\n",
    "We first splitted the data into test and train data. Then, we trained with the (same) train data the four models and we caluclated the scores that every model gave.\n",
    "\n",
    "As the label is binary, the results are either 0 or 1. We combined the 4 models by choosing for every row the most common value among the four models. And we had a tie, we took a random value.\n",
    "\n",
    "\n",
    "We did a grid search to find an approximate good hyperparameter for Decision Tree and Logistic Regression For computational issues, we couldn't do the same for the other models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4- Results\n",
    "\n",
    "Here are the results of different models : \n",
    " - Decision Tree :  0.73874883286648 ( Best ) ( With tuning, no overfitting ) \n",
    " - KNN ( without tuning ) : 0.734827264239029\n",
    " - RandomForestClassifier : 0.719327731092437 (without tuning, we notice that the train score is 0.99. It was clearly overfitted.)\n",
    " - Logistic Regression : 0.6728291316526611\n",
    "\n",
    "After combination of the 4 models : \n",
    " - Score :  0.7368814192343605 \n",
    " - F1 : 0.33 \n",
    " - Jaccard : 0.736\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5- Discusision\n",
    "\n",
    "The highest score we could achieve was around 0.73, we judge that it could be lifted up with model tuning. \n",
    "\n",
    "With a score of 0.73 without model tuning, we judge that the preprocessing of th data was pretty well done. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6- Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing of the data and feature engineering is a structural step in data science methodology. \n",
    "\n",
    "\n",
    "Every step of the data science methodology is important and no step can be ignored. We didn't tune all our models here and those tuned performed clearly better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
